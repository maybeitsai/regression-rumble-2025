{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47f83ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgbm\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df411697",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "807ef0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nilai yang hilang sebelum imputasi:\n",
      "seafloor_pressure                        4433\n",
      "total_alkalinity (µmol kg-1)             1512\n",
      "bottom_current_shear_stress (Pa)         1379\n",
      "sound_speed_water (m s-1)                2528\n",
      "Brunt_Vaisala_frequency_squared (s-2)    1676\n",
      "hydrostatic_pressure                     6567\n",
      "dtype: int64\n",
      "\n",
      "Evaluasi metode untuk total_alkalinity (µmol kg-1):\n",
      "Melakukan hyperparameter tuning untuk total_alkalinity (µmol kg-1) dengan Random Search...\n",
      "Parameter terbaik: {'colsample_bytree': 0.7801997007878172, 'learning_rate': 0.013846838736361293, 'max_depth': 11, 'min_child_samples': 64, 'n_estimators': 113, 'num_leaves': 28, 'reg_alpha': 0.04789875666064258, 'reg_lambda': 0.692681476866447, 'subsample': 0.6964101864104046}\n",
      "Top 5 fitur paling penting untuk total_alkalinity (µmol kg-1):\n",
      "                                  Feature  Importance\n",
      "40              sound_speed_water (m s-1)         178\n",
      "45  Brunt_Vaisala_frequency_squared (s-2)         130\n",
      "39       bottom_current_shear_stress (Pa)         121\n",
      "37        sea_surface_height_anomaly (cm)         110\n",
      "16        sediment_temperature_10_to_30cm         100\n",
      "  lgbm: RMSE = 20.8161, MAE = 16.5107, R² = -0.0025\n",
      "  knn: RMSE = 22.7693, MAE = 18.1798, R² = -0.1203\n",
      "  mice: RMSE = 20.7929, MAE = 16.5081, R² = -0.0003\n",
      "  Metode terbaik: mice dengan RMSE = 20.7929\n",
      "\n",
      "Evaluasi metode untuk bottom_current_shear_stress (Pa):\n",
      "Melakukan hyperparameter tuning untuk bottom_current_shear_stress (Pa) dengan Random Search...\n",
      "Parameter terbaik: {'colsample_bytree': 0.7334834444556088, 'learning_rate': 0.05143137719736283, 'max_depth': 5, 'min_child_samples': 26, 'n_estimators': 408, 'num_leaves': 21, 'reg_alpha': 2.165996316800474, 'reg_lambda': 2.8156581270472505, 'subsample': 0.6003115063364057}\n",
      "Top 5 fitur paling penting untuk bottom_current_shear_stress (Pa):\n",
      "                                 Feature  Importance\n",
      "29  chlorophyll_a_concentration (mg m-3)          74\n",
      "42                       turbidity (NTU)           4\n",
      "0                  water_temperature_50m           0\n",
      "35                                    pH           0\n",
      "26                     thermal_emissions           0\n",
      "  lgbm: RMSE = 0.0138, MAE = 0.0108, R² = -0.0003\n",
      "  knn: RMSE = 0.0150, MAE = 0.0116, R² = -0.1133\n",
      "  mice: RMSE = 0.0138, MAE = 0.0108, R² = -0.0033\n",
      "  Metode terbaik: lgbm dengan RMSE = 0.0138\n",
      "\n",
      "Evaluasi metode untuk sound_speed_water (m s-1):\n",
      "Melakukan hyperparameter tuning untuk sound_speed_water (m s-1) dengan Random Search...\n",
      "Parameter terbaik: {'colsample_bytree': 0.7801997007878172, 'learning_rate': 0.013846838736361293, 'max_depth': 11, 'min_child_samples': 64, 'n_estimators': 113, 'num_leaves': 28, 'reg_alpha': 0.04789875666064258, 'reg_lambda': 0.692681476866447, 'subsample': 0.6964101864104046}\n",
      "Top 5 fitur paling penting untuk sound_speed_water (m s-1):\n",
      "                                  Feature  Importance\n",
      "45  Brunt_Vaisala_frequency_squared (s-2)         179\n",
      "33           total_alkalinity (µmol kg-1)         151\n",
      "38        sea_surface_height_anomaly (cm)         142\n",
      "40       bottom_current_shear_stress (Pa)         140\n",
      "5                        plankton_density          99\n",
      "  lgbm: RMSE = 9.9018, MAE = 7.8313, R² = -0.0023\n",
      "  knn: RMSE = 10.8715, MAE = 8.6390, R² = -0.0675\n",
      "  mice: RMSE = 9.8915, MAE = 7.8269, R² = -0.0002\n",
      "  Metode terbaik: mice dengan RMSE = 9.8915\n",
      "\n",
      "Evaluasi metode untuk Brunt_Vaisala_frequency_squared (s-2):\n",
      "Melakukan hyperparameter tuning untuk Brunt_Vaisala_frequency_squared (s-2) dengan Random Search...\n",
      "Parameter terbaik: {'colsample_bytree': 0.749816047538945, 'learning_rate': 0.28570714885887566, 'max_depth': 10, 'min_child_samples': 65, 'n_estimators': 120, 'num_leaves': 122, 'reg_alpha': 1.3374982585607733, 'reg_lambda': 0.29992474745400866, 'subsample': 0.7836995567863468}\n",
      "Top 5 fitur paling penting untuk Brunt_Vaisala_frequency_squared (s-2):\n",
      "                  Feature  Importance\n",
      "0   water_temperature_50m           0\n",
      "1            salinity_50m           0\n",
      "26      thermal_emissions           0\n",
      "27         is_photic_zone           0\n",
      "28  photoperiod_intensity           0\n",
      "  lgbm: RMSE = 0.0001, MAE = 0.0001, R² = -0.0003\n",
      "  knn: RMSE = 0.0001, MAE = 0.0001, R² = -0.1400\n",
      "  mice: RMSE = 0.0001, MAE = 0.0001, R² = -0.0076\n",
      "  Metode terbaik: lgbm dengan RMSE = 0.0001\n",
      "\n",
      "Mengimputasi total_alkalinity (µmol kg-1) menggunakan mice...\n",
      "\n",
      "Mengimputasi bottom_current_shear_stress (Pa) menggunakan lgbm...\n",
      "Melakukan hyperparameter tuning untuk bottom_current_shear_stress (Pa) dengan Random Search...\n",
      "Parameter terbaik: {'colsample_bytree': 0.6588293923716152, 'learning_rate': 0.2787104112968334, 'max_depth': 3, 'min_child_samples': 88, 'n_estimators': 168, 'num_leaves': 53, 'reg_alpha': 1.8624170532662738, 'reg_lambda': 1.0422402246762426, 'subsample': 0.6836523164314057}\n",
      "Top 5 fitur paling penting untuk bottom_current_shear_stress (Pa):\n",
      "                                  Feature  Importance\n",
      "7                    mesoplankton_density           9\n",
      "14                     current_turbulence           9\n",
      "45  Brunt_Vaisala_frequency_squared (s-2)           5\n",
      "4                     sediment_deposition           4\n",
      "22         sediment_porosity_100_to_250cm           4\n",
      "\n",
      "Mengimputasi sound_speed_water (m s-1) menggunakan mice...\n",
      "\n",
      "Mengimputasi Brunt_Vaisala_frequency_squared (s-2) menggunakan lgbm...\n",
      "Melakukan hyperparameter tuning untuk Brunt_Vaisala_frequency_squared (s-2) dengan Random Search...\n",
      "Parameter terbaik: {'colsample_bytree': 0.749816047538945, 'learning_rate': 0.28570714885887566, 'max_depth': 10, 'min_child_samples': 65, 'n_estimators': 120, 'num_leaves': 122, 'reg_alpha': 1.3374982585607733, 'reg_lambda': 0.29992474745400866, 'subsample': 0.7836995567863468}\n",
      "Top 5 fitur paling penting untuk Brunt_Vaisala_frequency_squared (s-2):\n",
      "                  Feature  Importance\n",
      "0   water_temperature_50m           0\n",
      "1            salinity_50m           0\n",
      "26      thermal_emissions           0\n",
      "27         is_photic_zone           0\n",
      "28  photoperiod_intensity           0\n",
      "\n",
      "Nilai yang hilang setelah imputasi:\n",
      "seafloor_pressure       4433\n",
      "hydrostatic_pressure    6567\n",
      "dtype: int64\n",
      "\n",
      "Validasi hasil imputasi:\n",
      "  Kolom total_alkalinity (µmol kg-1) tidak memiliki nilai yang diimputasi\n",
      "  Kolom bottom_current_shear_stress (Pa) tidak memiliki nilai yang diimputasi\n",
      "  Kolom sound_speed_water (m s-1) tidak memiliki nilai yang diimputasi\n",
      "  Kolom Brunt_Vaisala_frequency_squared (s-2) tidak memiliki nilai yang diimputasi\n",
      "\n",
      "Data yang telah diimputasi disimpan ke: data/fitur_clean/other.csv\n"
     ]
    }
   ],
   "source": [
    "def validate_imputation(df, col_name):\n",
    "    \"\"\"\n",
    "    Memvalidasi hasil imputasi dengan memeriksa distribusi dan outlier\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame dengan data yang telah diimputasi\n",
    "        col_name: Nama kolom yang divalidasi\n",
    "    \"\"\"\n",
    "    if col_name not in df.columns:\n",
    "        print(f\"  Kolom {col_name} tidak ditemukan dalam data\")\n",
    "        return\n",
    "    \n",
    "    # Pisahkan data asli dan diimputasi\n",
    "    original_mask = df[col_name].notna()\n",
    "    original_data = df.loc[original_mask, col_name]\n",
    "    \n",
    "    # Jika semua data asli (tidak ada yang diimputasi), tidak perlu validasi\n",
    "    if len(original_data) == len(df):\n",
    "        print(f\"  Kolom {col_name} tidak memiliki nilai yang diimputasi\")\n",
    "        return\n",
    "    \n",
    "    # Statistik deskriptif untuk data asli\n",
    "    orig_mean = original_data.mean()\n",
    "    orig_std = original_data.std()\n",
    "    orig_min = original_data.min()\n",
    "    orig_max = original_data.max()\n",
    "    orig_median = original_data.median()\n",
    "    orig_q1 = original_data.quantile(0.25)\n",
    "    orig_q3 = original_data.quantile(0.75)\n",
    "    \n",
    "    # Statistik deskriptif untuk data keseluruhan (termasuk yang diimputasi)\n",
    "    all_mean = df[col_name].mean()\n",
    "    all_std = df[col_name].std()\n",
    "    all_min = df[col_name].min()\n",
    "    all_max = df[col_name].max()\n",
    "    all_median = df[col_name].median()\n",
    "    \n",
    "    # Cetak statistik perbandingan\n",
    "    print(f\"  Validasi untuk {col_name}:\")\n",
    "    print(f\"    Data asli: mean={orig_mean:.4f}, std={orig_std:.4f}, min={orig_min:.4f}, max={orig_max:.4f}, median={orig_median:.4f}\")\n",
    "    print(f\"    Data dengan imputasi: mean={all_mean:.4f}, std={all_std:.4f}, min={all_min:.4f}, max={all_max:.4f}, median={all_median:.4f}\")\n",
    "    \n",
    "    # Periksa apakah ada outlier ekstrem dalam data yang diimputasi\n",
    "    # Definisikan outlier sebagai nilai di luar 3x IQR dari data asli\n",
    "    iqr = orig_q3 - orig_q1\n",
    "    lower_bound = orig_q1 - 3 * iqr\n",
    "    upper_bound = orig_q3 + 3 * iqr\n",
    "    \n",
    "    # Data hasil imputasi\n",
    "    imputed_mask = ~original_mask\n",
    "    imputed_data = df.loc[imputed_mask, col_name]\n",
    "    \n",
    "    # Periksa jumlah outlier\n",
    "    n_outliers = ((imputed_data < lower_bound) | (imputed_data > upper_bound)).sum()\n",
    "    \n",
    "    if n_outliers > 0:\n",
    "        pct_outliers = (n_outliers / len(imputed_data)) * 100\n",
    "        print(f\"    Peringatan: {n_outliers} nilai hasil imputasi ({pct_outliers:.1f}%) berada di luar batas normal\")\n",
    "        \n",
    "        # Jika ada outlier ekstrem, lakukan clipping\n",
    "        if pct_outliers > 5:\n",
    "            print(f\"    Melakukan clipping pada outlier ekstrem...\")\n",
    "            df.loc[imputed_mask & (df[col_name] < lower_bound), col_name] = lower_bound\n",
    "            df.loc[imputed_mask & (df[col_name] > upper_bound), col_name] = upper_bound\n",
    "            print(f\"    Setelah clipping: min={df.loc[imputed_mask, col_name].min():.4f}, max={df.loc[imputed_mask, col_name].max():.4f}\")\n",
    "    else:\n",
    "        print(\"    Tidak ditemukan outlier dalam hasil imputasi\")\n",
    "    \n",
    "    # Periksa apakah distribusi data telah berubah secara signifikan\n",
    "    mean_diff_pct = abs((all_mean - orig_mean) / orig_mean) * 100\n",
    "    std_diff_pct = abs((all_std - orig_std) / orig_std) * 100\n",
    "    \n",
    "    if mean_diff_pct > 10 or std_diff_pct > 20:\n",
    "        print(f\"    Peringatan: Distribusi data mungkin telah berubah (mean diff={mean_diff_pct:.1f}%, std diff={std_diff_pct:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"    Distribusi data tetap konsisten setelah imputasi\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "# Asumsikan data sudah dimuat dalam DataFrame\n",
    "# Jika belum, gunakan code berikut untuk memuat data\n",
    "df = pd.read_csv('data/fitur_clean/anomaly.csv')\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Memuat data dari file CSV\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "def check_missing_values(df):\n",
    "    \"\"\"Menampilkan jumlah nilai yang hilang untuk setiap kolom\"\"\"\n",
    "    missing_values = df.isnull().sum()\n",
    "    return missing_values[missing_values > 0]\n",
    "\n",
    "def impute_total_alkalinity(df):\n",
    "    \"\"\"\n",
    "    Mengimputasi total_alkalinity berdasarkan salinitas dan suhu air\n",
    "    menggunakan rumus empiris\n",
    "    \"\"\"\n",
    "    # Membuat mask untuk mengidentifikasi nilai yang akan diimputasi\n",
    "    mask = df['total_alkalinity (µmol kg-1)'].isna()\n",
    "    \n",
    "    # Mengimputasi berdasarkan formula empiris\n",
    "    df.loc[mask, 'total_alkalinity (µmol kg-1)'] = (\n",
    "        2305 + \n",
    "        58.66 * (df.loc[mask, 'salinity_50m'] - 35) + \n",
    "        2.32 * (df.loc[mask, 'salinity_50m'] - 35)**2 - \n",
    "        1.41 * (df.loc[mask, 'water_temperature_50m'] - 20) + \n",
    "        0.04 * (df.loc[mask, 'water_temperature_50m'] - 20)**2\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def impute_bottom_current_shear_stress(df):\n",
    "    \"\"\"\n",
    "    Mengimputasi bottom_current_shear_stress berdasarkan\n",
    "    densitas air, koefisien drag, dan kecepatan arus dalam\n",
    "    \"\"\"\n",
    "    mask = df['bottom_current_shear_stress (Pa)'].isna()\n",
    "    \n",
    "    # Koefisien drag tipikal untuk dasar laut\n",
    "    Cd = 0.0025\n",
    "    \n",
    "    # Mengimputasi menggunakan rumus τ = ρ * Cd * U^2\n",
    "    df.loc[mask, 'bottom_current_shear_stress (Pa)'] = (\n",
    "        df.loc[mask, 'perceived_water_density'] * \n",
    "        Cd * \n",
    "        df.loc[mask, 'current_velocity_deep']**2\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def impute_sound_speed_water(df):\n",
    "    \"\"\"\n",
    "    Mengimputasi kecepatan suara dalam air menggunakan persamaan UNESCO\n",
    "    \"\"\"\n",
    "    mask = df['sound_speed_water (m s-1)'].isna()\n",
    "    \n",
    "    # Menggunakan kedalaman untuk perhitungan (estimasi dari hydrostatic_pressure)\n",
    "    # Jika hydrostatic_pressure tidak tersedia, gunakan nilai default untuk D\n",
    "    if 'hydrostatic_pressure' in df.columns and not df['hydrostatic_pressure'].isna().all():\n",
    "        # Estimasi kedalaman dari tekanan hidrostatik (approx: 1 dbar ≈ 1 meter)\n",
    "        D = df.loc[mask, 'hydrostatic_pressure'] / 10000  # konversi ke dbar\n",
    "    else:\n",
    "        # Nilai default kedalaman jika tidak ada data tekanan\n",
    "        D = 50  # asumsi kedalaman 50m berdasarkan beberapa kolom yang diukur pada 50m\n",
    "    \n",
    "    # Imputasi menggunakan rumus UNESCO\n",
    "    T = df.loc[mask, 'water_temperature_50m']\n",
    "    S = df.loc[mask, 'salinity_50m']\n",
    "    \n",
    "    df.loc[mask, 'sound_speed_water (m s-1)'] = (\n",
    "        1449.2 + 4.6*T - 0.055*T**2 + 0.00029*T**3 + \n",
    "        (1.34 - 0.01*T)*(S - 35) + 0.016*D\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def impute_brunt_vaisala(df):\n",
    "    \"\"\"\n",
    "    Mengimputasi Brunt-Väisälä frequency squared berdasarkan\n",
    "    densitas air dan mixed layer depth\n",
    "    \"\"\"\n",
    "    mask = df['Brunt_Vaisala_frequency_squared (s-2)'].isna()\n",
    "    \n",
    "    # Gravitasi\n",
    "    g = 9.81  # m/s^2\n",
    "    \n",
    "    # Estimasi gradien densitas vertikal\n",
    "    # Karena kita hanya memiliki satu pengukuran densitas, kita perlu membuat estimasi\n",
    "    # Kita dapat menggunakan mixed_layer_depth dan asumsi gradien densitas\n",
    "    \n",
    "    # Perbedaan densitas tipikal antara permukaan dan bawah mixed layer\n",
    "    delta_rho = 0.5  # kg/m^3, nilai tipikal untuk stratifikasi laut\n",
    "    \n",
    "    # Estimasi N^2 = (g/ρ) * (dρ/dz)\n",
    "    df.loc[mask, 'Brunt_Vaisala_frequency_squared (s-2)'] = (\n",
    "        g / df.loc[mask, 'perceived_water_density'] * \n",
    "        (delta_rho / df.loc[mask, 'mixed_layer_depth (m)'])\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def impute_with_lgbm(df, target_column, n_iterations=20, cv_folds=3):\n",
    "    \"\"\"\n",
    "    Mengimputasi nilai menggunakan LightGBM dengan hyperparameter tuning melalui Random Search\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame dengan data\n",
    "        target_column: Nama kolom yang akan diimputasi\n",
    "        n_iterations: Jumlah iterasi untuk Random Search\n",
    "        cv_folds: Jumlah lipatan cross-validation\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame dengan nilai yang telah diimputasi\n",
    "    \"\"\"\n",
    "    # Membuat salinan data\n",
    "    df_temp = df.copy()\n",
    "    \n",
    "    # Identifikasi nilai yang hilang\n",
    "    mask = df_temp[target_column].isna()\n",
    "    \n",
    "    # Jika tidak ada nilai yang hilang, kembalikan dataframe asli\n",
    "    if not mask.any():\n",
    "        return df\n",
    "    \n",
    "    # Pilih fitur untuk model\n",
    "    # Hindari kolom dengan nilai yang hilang dan kolom yang tidak relevan\n",
    "    exclude_cols = ['measurement_id', 'depth_reading_time', 'hydrostatic_pressure', \n",
    "                    'seafloor_pressure', 'year', 'month', 'day', 'hour', 'dayofweek',\n",
    "                    target_column]\n",
    "    \n",
    "    # Tambahkan kolom dengan missing values tinggi ke exclude_cols\n",
    "    for col in df.columns:\n",
    "        if col not in exclude_cols and df[col].isna().sum() > 0.3 * len(df):\n",
    "            exclude_cols.append(col)\n",
    "    \n",
    "    feature_cols = [col for col in df_temp.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Bagi data menjadi set pelatihan dan set target\n",
    "    df_train = df_temp[~mask]\n",
    "    X_train = df_train[feature_cols]\n",
    "    y_train = df_train[target_column]\n",
    "    \n",
    "    # Definisi rentang hyperparameter untuk random search\n",
    "    param_distributions = {\n",
    "        'num_leaves': randint(20, 200),\n",
    "        'learning_rate': uniform(0.01, 0.29),\n",
    "        'n_estimators': randint(100, 500),\n",
    "        'max_depth': randint(3, 12),\n",
    "        'min_child_samples': randint(5, 100),\n",
    "        'subsample': uniform(0.6, 0.4),\n",
    "        'colsample_bytree': uniform(0.6, 0.4),\n",
    "        'reg_alpha': uniform(0, 3),\n",
    "        'reg_lambda': uniform(0, 3),\n",
    "    }\n",
    "    \n",
    "    # Buat model LightGBM dasar\n",
    "    base_model = lgbm.LGBMRegressor(\n",
    "        objective='regression',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=-1,\n",
    "        importance_type='gain'\n",
    "    )\n",
    "    \n",
    "    # Lakukan random search untuk hyperparameter tuning\n",
    "    print(f\"Melakukan hyperparameter tuning untuk {target_column} dengan Random Search...\")\n",
    "    rs = RandomizedSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=n_iterations,\n",
    "        cv=cv_folds,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        verbose=0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Train model dengan hyperparameter terbaik\n",
    "    rs.fit(X_train, y_train)\n",
    "    \n",
    "    # Tampilkan parameter terbaik\n",
    "    best_params = rs.best_params_\n",
    "    print(f\"Parameter terbaik: {best_params}\")\n",
    "    \n",
    "    # Buat model dengan parameter terbaik\n",
    "    best_model = lgbm.LGBMRegressor(**best_params, random_state=42)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Fitur importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(f\"Top 5 fitur paling penting untuk {target_column}:\")\n",
    "    print(feature_importance.head(5))\n",
    "    \n",
    "    # Prediksi nilai yang hilang\n",
    "    X_missing = df_temp.loc[mask, feature_cols]\n",
    "    df.loc[mask, target_column] = best_model.predict(X_missing)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def impute_with_knn(df, target_column, n_neighbors=5):\n",
    "    \"\"\"\n",
    "    Mengimputasi nilai menggunakan KNN Imputer\n",
    "    \"\"\"\n",
    "    # Pilih fitur untuk model (hindari kolom dengan missing values dan kolom non-numerik)\n",
    "    exclude_cols = ['measurement_id', 'depth_reading_time', 'hydrostatic_pressure', \n",
    "                  'seafloor_pressure', 'year', 'month', 'day', 'hour', 'dayofweek']\n",
    "    \n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols and \n",
    "                   df[col].dtype in ['float64', 'int64'] and \n",
    "                   col != target_column]\n",
    "    \n",
    "    # Skalakan fitur\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(df[feature_cols]),\n",
    "        columns=feature_cols\n",
    "    )\n",
    "    \n",
    "    # Tambahkan kolom target\n",
    "    df_scaled[target_column] = df[target_column]\n",
    "    \n",
    "    # Imputasi KNN\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    df_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(df_scaled),\n",
    "        columns=df_scaled.columns\n",
    "    )\n",
    "    \n",
    "    # Kembalikan nilai yang diimputasi ke dataframe asli\n",
    "    df[target_column] = df_imputed[target_column]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def impute_with_mice(df, target_columns):\n",
    "    \"\"\"\n",
    "    Mengimputasi nilai menggunakan Multiple Imputation by Chained Equations (MICE)\n",
    "    \"\"\"\n",
    "    # Pilih kolom untuk imputasi\n",
    "    feature_cols = [col for col in df.columns if df[col].dtype in ['float64', 'int64'] and\n",
    "                   col not in ['measurement_id', 'depth_reading_time', 'hydrostatic_pressure', 'seafloor_pressure']]\n",
    "    \n",
    "    # Inisialisasi MICE\n",
    "    mice_imputer = IterativeImputer(max_iter=15, random_state=42)\n",
    "    \n",
    "    # Lakukan imputasi\n",
    "    df_imputed = df.copy()\n",
    "    df_imputed[feature_cols] = mice_imputer.fit_transform(df[feature_cols])\n",
    "    \n",
    "    # Kembalikan nilai yang diimputasi untuk kolom target ke dataframe asli\n",
    "    for col in target_columns:\n",
    "        if col in feature_cols:\n",
    "            df[col] = df_imputed[col]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def evaluate_imputation(df, target_column, imputation_method, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Mengevaluasi metode imputasi dengan menyembunyikan sebagian data yang ada \n",
    "    dan membandingkan nilai prediksi dengan nilai sebenarnya\n",
    "    \"\"\"\n",
    "    # Salin data yang memiliki nilai (tidak NA) di kolom target\n",
    "    df_complete = df[df[target_column].notna()].copy()\n",
    "    \n",
    "    # Bagi data menjadi training dan testing\n",
    "    train_data, test_data = train_test_split(df_complete, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Salin nilai asli dari data pengujian\n",
    "    original_values = test_data[target_column].copy()\n",
    "    \n",
    "    # Set nilai kolom target pada data pengujian menjadi NA\n",
    "    test_data[target_column] = np.nan\n",
    "    \n",
    "    # Gabungkan data pelatihan dan pengujian\n",
    "    combined_data = pd.concat([train_data, test_data])\n",
    "    \n",
    "    # Lakukan imputasi\n",
    "    if imputation_method == 'lgbm':\n",
    "        imputed_data = impute_with_lgbm(combined_data, target_column, n_iterations=10, cv_folds=3)\n",
    "    elif imputation_method == 'knn':\n",
    "        imputed_data = impute_with_knn(combined_data, target_column)\n",
    "    elif imputation_method == 'mice':\n",
    "        imputed_data = impute_with_mice(combined_data, [target_column])\n",
    "    else:\n",
    "        raise ValueError(\"Metode imputasi tidak valid\")\n",
    "    \n",
    "    # Ambil nilai yang diimputasi\n",
    "    imputed_values = imputed_data.loc[test_data.index, target_column]\n",
    "    \n",
    "    # Hitung metrik evaluasi\n",
    "    mse = np.mean((original_values - imputed_values) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(original_values - imputed_values))\n",
    "    r2 = 1 - (np.sum((original_values - imputed_values) ** 2) / \n",
    "              np.sum((original_values - np.mean(original_values)) ** 2))\n",
    "    \n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2\n",
    "    }\n",
    "\n",
    "def main(file_path):\n",
    "    \"\"\"\n",
    "    Fungsi utama untuk memproses dan mengimputasi data\n",
    "    \"\"\"\n",
    "    # Muat data\n",
    "    df = load_data(file_path)\n",
    "    \n",
    "    # Periksa nilai yang hilang sebelum imputasi\n",
    "    print(\"Nilai yang hilang sebelum imputasi:\")\n",
    "    print(check_missing_values(df))\n",
    "    \n",
    "    # Daftar kolom yang perlu diimputasi\n",
    "    columns_to_impute = [\n",
    "        'total_alkalinity (µmol kg-1)',\n",
    "        'bottom_current_shear_stress (Pa)',\n",
    "        'sound_speed_water (m s-1)',\n",
    "        'Brunt_Vaisala_frequency_squared (s-2)'\n",
    "    ]\n",
    "    \n",
    "    # Evaluasi dan pilih metode terbaik untuk setiap kolom\n",
    "    results = {}\n",
    "    methods = ['lgbm', 'knn', 'mice']\n",
    "    \n",
    "    for col in columns_to_impute:\n",
    "        # Jika kolom memiliki cukup data untuk dievaluasi\n",
    "        if df[col].notna().sum() > 100:  # Minimal 100 data untuk evaluasi\n",
    "            print(f\"\\nEvaluasi metode untuk {col}:\")\n",
    "            col_results = {}\n",
    "            \n",
    "            for method in methods:\n",
    "                try:\n",
    "                    result = evaluate_imputation(df, col, method)\n",
    "                    col_results[method] = result\n",
    "                    print(f\"  {method}: RMSE = {result['RMSE']:.4f}, MAE = {result['MAE']:.4f}, R² = {result['R2']:.4f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  {method}: Error - {str(e)}\")\n",
    "            \n",
    "            # Pilih metode dengan RMSE terendah\n",
    "            best_method = min(col_results, key=lambda x: col_results[x]['RMSE'])\n",
    "            results[col] = {'best_method': best_method, 'metrics': col_results[best_method]}\n",
    "            print(f\"  Metode terbaik: {best_method} dengan RMSE = {col_results[best_method]['RMSE']:.4f}\")\n",
    "        else:\n",
    "            # Jika data tidak cukup untuk evaluasi, gunakan pendekatan berbasis fisika\n",
    "            results[col] = {'best_method': 'physics_based', 'metrics': None}\n",
    "            print(f\"\\n{col}: Menggunakan pendekatan berbasis fisika karena data tidak cukup untuk evaluasi\")\n",
    "    \n",
    "    # Lakukan imputasi menggunakan metode terbaik atau metode berbasis fisika\n",
    "    for col in columns_to_impute:\n",
    "        print(f\"\\nMengimputasi {col} menggunakan {results[col]['best_method']}...\")\n",
    "        \n",
    "        if results[col]['best_method'] == 'physics_based':\n",
    "            if col == 'total_alkalinity (µmol kg-1)':\n",
    "                df = impute_total_alkalinity(df)\n",
    "            elif col == 'bottom_current_shear_stress (Pa)':\n",
    "                df = impute_bottom_current_shear_stress(df)\n",
    "            elif col == 'sound_speed_water (m s-1)':\n",
    "                df = impute_sound_speed_water(df)\n",
    "            elif col == 'Brunt_Vaisala_frequency_squared (s-2)':\n",
    "                df = impute_brunt_vaisala(df)\n",
    "        elif results[col]['best_method'] == 'lgbm':\n",
    "            # Untuk LGBM final, gunakan lebih banyak iterasi untuk random search\n",
    "            df = impute_with_lgbm(df, col, n_iterations=90, cv_folds=5)\n",
    "        elif results[col]['best_method'] == 'knn':\n",
    "            df = impute_with_knn(df, col)\n",
    "        elif results[col]['best_method'] == 'mice':\n",
    "            df = impute_with_mice(df, [col])\n",
    "    \n",
    "    # Periksa nilai yang hilang setelah imputasi\n",
    "    print(\"\\nNilai yang hilang setelah imputasi:\")\n",
    "    print(check_missing_values(df))\n",
    "    \n",
    "    # Validasi hasil imputasi\n",
    "    print(\"\\nValidasi hasil imputasi:\")\n",
    "    for col in columns_to_impute:\n",
    "        validate_imputation(df, col)\n",
    "    \n",
    "    # Simpan hasil\n",
    "    output_path = \"data/fitur_clean/other.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nData yang telah diimputasi disimpan ke: {output_path}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Contoh penggunaan:\n",
    "if __name__ == \"__main__\":\n",
    "    # Ganti dengan path file CSV Anda\n",
    "    file_path = \"data/fitur_clean/anomaly.csv\"  \n",
    "    imputed_df = main(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analisis-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
