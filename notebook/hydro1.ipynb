{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efe2ec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Standard Library =====\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "# ===== Data Science Libraries =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import loguniform, norm, randint, uniform\n",
    "\n",
    "# ===== Visualization =====\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ===== Scikit-learn =====\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import (\n",
    "    LabelEncoder,\n",
    "    PolynomialFeatures,\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    StandardScaler,\n",
    "    RobustScaler\n",
    ")\n",
    "\n",
    "# Imputation\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "\n",
    "# Model selection & evaluation\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    KFold,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    ")\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Feature engineering\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# Model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# ===== External ML Libraries =====\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import shap\n",
    "\n",
    "# ===== Statistical Tools =====\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# ===== Utility =====\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='lightgbm')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# -- Path File  --\n",
    "TRAIN_PATH = 'data/regression-rumble-ndc-2025/train.csv'\n",
    "TEST_PATH = 'data/regression-rumble-ndc-2025/test.csv'\n",
    "SUBMISSION_PATH = 'data/regression-rumble-ndc-2025/sample_submission.csv'\n",
    "\n",
    "# --- Konstanta ---\n",
    "SEED = 42\n",
    "TARGET = 'hydrostatic_pressure'  # Switched target to hydrostatic_pressure\n",
    "N_SPLITS = 10\n",
    "EPSILON = 1e-6\n",
    "N_SPLITS_CV = 5\n",
    "TARGET_R2 = 0.99\n",
    "\n",
    "# -- Konfigurasi Dasar --\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f1a37",
   "metadata": {},
   "source": [
    "### Hydrostatic 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccf77cb",
   "metadata": {},
   "source": [
    "#### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5587af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "LGBM_PARAMS_COMMON = {\n",
    "    'random_state': SEED,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1, # Suppress LightGBM verbosity during training\n",
    "}\n",
    "# Expanded parameter grid for LightGBM hyperparameter tuning\n",
    "LGBM_PARAM_GRID = {\n",
    "    'n_estimators': [2000, 2500, 3000],         \n",
    "    'learning_rate': [0.01, 0.03, 0.05],      \n",
    "    'num_leaves': [31, 63, 127],                \n",
    "    'max_depth': [7, 8, 9, 10],               \n",
    "    'min_child_samples': [5, 20, 50],         \n",
    "    'subsample': [0.7, 0.8, 0.9],                \n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],         \n",
    "    'reg_alpha': [0, 0.1, 0.5, 1.0],             \n",
    "    'reg_lambda': [0, 0.1, 0.5, 1.0],           \n",
    "    'min_split_gain': [0, 0.1, 0.2],               \n",
    "    'bagging_freq': [0, 1, 5],                    \n",
    "    'feature_fraction_seed': [SEED]                \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2226ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "def create_datetime_features(df, column_name):\n",
    "    df_copy = df.copy()\n",
    "    if column_name not in df_copy.columns:\n",
    "        print(f\"Warning: Datetime column '{column_name}' not found for feature extraction.\")\n",
    "        return df_copy\n",
    "        \n",
    "    # Ensure the column is actually datetime before attempting dt accessor\n",
    "    if pd.api.types.is_datetime64_any_dtype(df_copy[column_name]):\n",
    "        df_copy['month'] = df_copy[column_name].dt.month\n",
    "        df_copy['day'] = df_copy[column_name].dt.day\n",
    "        df_copy['hour'] = df_copy[column_name].dt.hour\n",
    "        df_copy['dayofweek'] = df_copy[column_name].dt.dayofweek\n",
    "        df_copy['weekofyear'] = df_copy[column_name].dt.isocalendar().week.astype(int)\n",
    "        df_copy['quarter'] = df_copy[column_name].dt.quarter\n",
    "        # df_copy = df_copy.drop(columns=[column_name]) # Drop original datetime column\n",
    "    else:\n",
    "        print(f\"Warning: Column '{column_name}' is not datetime type. Skipping datetime feature extraction.\")\n",
    "    return df_copy\n",
    "\n",
    "def advanced_feature_engineering(df_in):\n",
    "    df = df_in.copy()\n",
    "    epsilon = 1e-6 # For safe division\n",
    "    \n",
    "    # Interactions for temperature and salinity at different depths\n",
    "    depth_suffixes = ['0m', '10m', '20m', '30m', '40m', '50m']\n",
    "    for suffix in depth_suffixes:\n",
    "        temp_col = f'water_temperature_{suffix}'\n",
    "        sal_col = f'salinity_{suffix}'\n",
    "        if temp_col in df.columns and sal_col in df.columns:\n",
    "            # Ensure columns are numeric before operation\n",
    "            df[f'temp_sal_interaction_{suffix}'] = pd.to_numeric(df[temp_col], errors='coerce') * pd.to_numeric(df[sal_col], errors='coerce')\n",
    "\n",
    "    # Gradients between surface and 50m (if available)\n",
    "    for param in ['water_temperature', 'salinity', 'dissolved_oxygen', 'ph', 'turbidity', 'chlorophyll']:\n",
    "        col_0m = f'{param}_0m'\n",
    "        col_50m = f'{param}_50m'\n",
    "        if col_0m in df.columns and col_50m in df.columns:\n",
    "            df[f'{param}_grad_0_50m'] = pd.to_numeric(df[col_0m], errors='coerce') - pd.to_numeric(df[col_50m], errors='coerce')\n",
    "\n",
    "    # Ratios (add epsilon to avoid division by zero)\n",
    "    if 'water_temperature_0m' in df.columns and 'water_temperature_50m' in df.columns:\n",
    "         df['temp_ratio_0_50m'] = pd.to_numeric(df['water_temperature_0m'], errors='coerce') / (pd.to_numeric(df['water_temperature_50m'], errors='coerce') + epsilon)\n",
    "    if 'salinity_0m' in df.columns and 'salinity_50m' in df.columns:\n",
    "        df['sal_ratio_0_50m'] = pd.to_numeric(df['salinity_0m'], errors='coerce') / (pd.to_numeric(df['salinity_50m'], errors='coerce') + epsilon)\n",
    "\n",
    "    # Polynomials of key features\n",
    "    if 'depth_m' in df.columns and pd.api.types.is_numeric_dtype(df['depth_m']):\n",
    "        df['depth_m_sq'] = df['depth_m']**2\n",
    "        df['depth_m_cub'] = df['depth_m']**3\n",
    "        \n",
    "    # If seafloor_pressure is a feature (relevant for our feature engineering path)\n",
    "    if 'seafloor_pressure' in df.columns and pd.api.types.is_numeric_dtype(df['seafloor_pressure']):\n",
    "        df['seafloor_pressure_sq'] = df['seafloor_pressure']**2\n",
    "        if 'depth_m' in df.columns and pd.api.types.is_numeric_dtype(df['depth_m']):\n",
    "            df['seafloor_pressure_per_depth'] = df['seafloor_pressure'] / (df['depth_m'] + epsilon)\n",
    "\n",
    "    # Mean/std of related features across depths\n",
    "    param_groups_data = {\n",
    "        'water_temperature': [f'water_temperature_{s}' for s in depth_suffixes],\n",
    "        'salinity': [f'salinity_{s}' for s in depth_suffixes]\n",
    "        # Add other param groups if relevant\n",
    "    }\n",
    "    for param_name, param_group_cols in param_groups_data.items():\n",
    "        existing_cols = [col for col in param_group_cols if col in df.columns]\n",
    "        if len(existing_cols) > 1: \n",
    "            # Convert to numeric before aggregation\n",
    "            numeric_series_list = [pd.to_numeric(df[col], errors='coerce') for col in existing_cols]\n",
    "            temp_df_for_agg = pd.concat(numeric_series_list, axis=1)\n",
    "            \n",
    "            df[f'{param_name}_mean_profile'] = temp_df_for_agg.mean(axis=1)\n",
    "            df[f'{param_name}_std_profile'] = temp_df_for_agg.std(axis=1)\n",
    "            df[f'{param_name}_range_profile'] = temp_df_for_agg.max(axis=1) - temp_df_for_agg.min(axis=1)\n",
    "    return df\n",
    "\n",
    "def preprocess_data_for_model(data_df, \n",
    "                              engineered_feature_columns_to_use, # List of feature names from training\n",
    "                              is_training_data, \n",
    "                              for_model_A=True): # Flag to distinguish FE paths if needed\n",
    "    df_processed = data_df.copy()\n",
    "    \n",
    "    # Apply advanced feature engineering\n",
    "    # If for_model_A is False, 'seafloor_pressure'-dependent features should be handled differently\n",
    "    df_processed = advanced_feature_engineering(df_processed)\n",
    "    \n",
    "    if is_training_data:\n",
    "        # For training, select all numeric columns (original + engineered)\n",
    "        df_final_features = df_processed.select_dtypes(include=np.number)\n",
    "        # Store the columns used for training\n",
    "        current_training_features = df_final_features.columns.tolist()\n",
    "        return df_final_features, current_training_features\n",
    "    else:\n",
    "        # For prediction data\n",
    "        # Ensure all engineered_feature_columns_to_use are present, add missing ones with NaN\n",
    "        for col in engineered_feature_columns_to_use:\n",
    "            if col not in df_processed.columns:\n",
    "                df_processed[col] = np.nan # Add missing feature as NaN\n",
    "        # Select and reorder to match training feature set\n",
    "        # Ensure only features from the training list are used, in the correct order\n",
    "        df_final_features = df_processed[engineered_feature_columns_to_use].copy() \n",
    "        df_final_features = df_final_features.select_dtypes(include=np.number) # Final check for numeric\n",
    "        return df_final_features, None # No new training features list for prediction data\n",
    "\n",
    "def train_lgbm_model(X_train, y_train, model_name):\n",
    "    \"\"\"\n",
    "    Train LightGBM model using RandomizedSearchCV for more comprehensive hyperparameter tuning\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Training {model_name} ---\")\n",
    "    if X_train.empty or y_train.empty:\n",
    "        print(f\"Training data for {model_name} is empty. Skipping training.\")\n",
    "        return None, [], -1.0  # model, features_used_list, best_cv_score\n",
    "\n",
    "    # Create LightGBM model with common parameters\n",
    "    lgbm = lgb.LGBMRegressor(**LGBM_PARAMS_COMMON)\n",
    "    kf = KFold(n_splits=N_SPLITS_CV, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    print(f\"Performing RandomizedSearchCV for {model_name} with {X_train.shape[1]} features...\")\n",
    "    # Using RandomizedSearchCV instead of GridSearchCV for faster execution with more parameters\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=lgbm, \n",
    "        param_distributions=LGBM_PARAM_GRID, \n",
    "        n_iter=90,\n",
    "        cv=kf, \n",
    "        scoring='r2', \n",
    "        verbose=1, \n",
    "        n_jobs=-1,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = random_search.best_estimator_\n",
    "    best_params = random_search.best_params_\n",
    "    best_cv_r2_score = random_search.best_score_\n",
    "    \n",
    "    print(f\"Best parameters for {model_name}: {best_params}\")\n",
    "    print(f\"Best cross-validated R² score for {model_name}: {best_cv_r2_score:.4f}\")\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    if best_model is not None:\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'Importance': best_model.feature_importances_\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        print(\"\\nTop 10 Feature Importances:\")\n",
    "        print(feature_importance.head(10))\n",
    "        \n",
    "    return best_model, X_train.columns.tolist(), best_cv_r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51110f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 1: Preprocessing & Setup ---\n",
      "Loaded dataset with shape: (21888, 60)\n",
      "Size of Subset A (initially missing hydrostatic_pressure, seafloor_pressure available): 5278\n",
      "Size of Subset B (initially missing both hydrostatic_pressure and seafloor_pressure): 1289\n",
      "Cleaning 'oxygen_saturation_50m'...\n",
      "Shape after initial cleaning and datetime features: (21888, 60)\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 1: Preprocessing & Setup ---\n",
    "print(\"\\n--- STEP 1: Preprocessing & Setup ---\")\n",
    "csv_path = 'data/fitur_clean/seafloor1.csv'\n",
    "df = pd.read_csv(csv_path, parse_dates=['depth_reading_time'])\n",
    "print(f\"Loaded dataset with shape: {df.shape}\")\n",
    "\n",
    "# Store original indices for Subset A and B based on initial NaN status\n",
    "missing_hydro_pressure_mask_initial = df['hydrostatic_pressure'].isnull()\n",
    "subset_A_indices = df[missing_hydro_pressure_mask_initial & df['seafloor_pressure'].notnull()].index\n",
    "subset_B_indices = df[missing_hydro_pressure_mask_initial & df['seafloor_pressure'].isnull()].index\n",
    "print(f\"Size of Subset A (initially missing hydrostatic_pressure, seafloor_pressure available): {len(subset_A_indices)}\")\n",
    "print(f\"Size of Subset B (initially missing both hydrostatic_pressure and seafloor_pressure): {len(subset_B_indices)}\")\n",
    "\n",
    "# Clean `oxygen_saturation_50m`\n",
    "if 'oxygen_saturation_50m' in df.columns:\n",
    "    print(\"Cleaning 'oxygen_saturation_50m'...\")\n",
    "    df['oxygen_saturation_50m'] = df['oxygen_saturation_50m'].astype(str).str.replace(',', '.', regex=False)\n",
    "    df['oxygen_saturation_50m'] = pd.to_numeric(df['oxygen_saturation_50m'], errors='coerce')\n",
    "\n",
    "# Extract datetime features (and drop original datetime column)\n",
    "df = create_datetime_features(df, 'depth_reading_time')\n",
    "print(f\"Shape after initial cleaning and datetime features: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93007c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 2: Model A (with seafloor_pressure) ---\n",
      "Shape of X_A_train for Model A: (15321, 60), y_A_train: (15321,)\n",
      "\n",
      "--- Training Model A ---\n",
      "Performing RandomizedSearchCV for Model A with 60 features...\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "Best parameters for Model A: {'subsample': 0.7, 'reg_lambda': 0, 'reg_alpha': 0, 'num_leaves': 63, 'n_estimators': 3000, 'min_split_gain': 0, 'min_child_samples': 5, 'max_depth': 10, 'learning_rate': 0.03, 'feature_fraction_seed': 42, 'colsample_bytree': 0.8, 'bagging_freq': 1}\n",
      "Best cross-validated R² score for Model A: 0.9995\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "                                  Feature  Importance\n",
      "6                       seafloor_pressure        7594\n",
      "4                 perceived_water_density        5213\n",
      "42       bottom_current_shear_stress (Pa)        4958\n",
      "40        sea_surface_height_anomaly (cm)        4832\n",
      "35           total_alkalinity (µmol kg-1)        4828\n",
      "48  Brunt_Vaisala_frequency_squared (s-2)        4821\n",
      "1                   water_temperature_50m        4818\n",
      "43              sound_speed_water (m s-1)        4774\n",
      "34      silicate_concentration (µmol L-1)        4294\n",
      "11                 dissolved_gas_pressure        4282\n",
      "Model A performance is high (CV R²: 0.9995 >= 0.99). Performing early imputation for Subset A.\n",
      "Imputed 5278 values in Subset A (early).\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 2: Model A – Predict `hydrostatic_pressure` (with `seafloor_pressure`) ---\n",
    "print(\"\\n--- STEP 2: Model A (with seafloor_pressure) ---\")\n",
    "model_A = None\n",
    "model_A_features = [] # To store feature names used by Model A\n",
    "cv_r2_A = -1.0 # Initialize with a value indicating not run or failed\n",
    "subset_A_early_imputed = False\n",
    "\n",
    "# Training data for Model A: hydrostatic_pressure NOT NULL, seafloor_pressure NOT NULL\n",
    "train_A_df_mask = df['hydrostatic_pressure'].notnull() & df['seafloor_pressure'].notnull()\n",
    "train_A_df = df[train_A_df_mask].copy() # Use .copy() to avoid SettingWithCopyWarning later\n",
    "\n",
    "if not train_A_df.empty:\n",
    "    y_A_train = train_A_df['hydrostatic_pressure']\n",
    "    X_A_train_raw = train_A_df.drop(columns=['hydrostatic_pressure']) \n",
    "    \n",
    "    # Preprocess features for Model A training\n",
    "    X_A_train, model_A_features_discovered = preprocess_data_for_model(\n",
    "        X_A_train_raw, None, is_training_data=True, for_model_A=True\n",
    "    )\n",
    "    model_A_features = model_A_features_discovered # Store for prediction phase\n",
    "    \n",
    "    print(f\"Shape of X_A_train for Model A: {X_A_train.shape}, y_A_train: {y_A_train.shape}\")\n",
    "    model_A, _, cv_r2_A = train_lgbm_model(X_A_train, y_A_train, \"Model A\")\n",
    "\n",
    "    # Conditional early imputation for Subset A\n",
    "    if model_A and cv_r2_A >= TARGET_R2:\n",
    "        if not len(subset_A_indices) == 0:\n",
    "            print(f\"Model A performance is high (CV R²: {cv_r2_A:.4f} >= {TARGET_R2}). Performing early imputation for Subset A.\")\n",
    "            subset_A_data_raw_predict = df.loc[subset_A_indices].drop(columns=['hydrostatic_pressure'], errors='ignore').copy()\n",
    "            \n",
    "            subset_A_features_predict, _ = preprocess_data_for_model(\n",
    "                subset_A_data_raw_predict, model_A_features, is_training_data=False, for_model_A=True\n",
    "            )\n",
    "            \n",
    "            predictions_A_early = model_A.predict(subset_A_features_predict)\n",
    "            df.loc[subset_A_indices, 'hydrostatic_pressure'] = predictions_A_early\n",
    "            subset_A_early_imputed = True\n",
    "            print(f\"Imputed {len(predictions_A_early)} values in Subset A (early).\")\n",
    "        else:\n",
    "            print(\"Subset A is empty, no early imputation by Model A needed.\")\n",
    "    elif model_A: # Model trained but R2 not high enough for early imputation\n",
    "        print(f\"Model A performance (CV R²: {cv_r2_A:.4f}) is below target {TARGET_R2}. Subset A will be imputed later in Step 3 if still NaN.\")\n",
    "    else: # Model A not trained\n",
    "        print(\"Model A not trained. Cannot perform early imputation for Subset A.\")\n",
    "else:\n",
    "    print(\"No training data available for Model A. Skipping Model A training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f0768",
   "metadata": {},
   "source": [
    "#### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c5a346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration for CatBoost ---\n",
    "CATBOOST_PARAMS_COMMON = {\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 0,  # Suppress CatBoost verbosity during training\n",
    "    'thread_count': -1,\n",
    "}\n",
    "# Parameter grid for CatBoost hyperparameter tuning - optimized\n",
    "CATBOOST_PARAM_GRID = {\n",
    "    'iterations': [2000, 2500, 3000],                           # Number of trees to build\n",
    "    'learning_rate': [0.01, 0.03, 0.05],                  # Multiple learning rates for better exploration\n",
    "    'depth': [7, 8, 9, 10],                                  # Tree depth - crucial for model complexity\n",
    "    'l2_leaf_reg': [1, 3, 5, 7],                          # L2 regularization to prevent overfitting\n",
    "    'border_count': [128, 254],                           # Number of splits for numerical features\n",
    "    'bagging_temperature': [0.7, 1.0, 1.5],               # Controls randomness in bagging (higher = more random)\n",
    "    'random_strength': [0.1, 1.0, 3.0],                   # Amount of randomness to use for scoring splits\n",
    "    'min_data_in_leaf': [1, 10, 20],                      # Minimum observations needed in a leaf\n",
    "    'leaf_estimation_method': ['Newton', 'Gradient'],     # Method used to calculate leaf values\n",
    "    'grow_policy': ['SymmetricTree', 'Depthwise', 'Lossguide'], # Method of tree construction\n",
    "    'subsample': [0.66, 0.8, 0.95],                       # Sample rate for Bernoulli bootstrap\n",
    "    'od_type': ['Iter'],                                  # Type of overfitting detector\n",
    "    'od_wait': [100]                                      # Early stopping rounds\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bf9674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_catboost_model(X_train, y_train, model_name):\n",
    "    print(f\"\\n--- Training {model_name} ---\")\n",
    "    if X_train.empty or y_train.empty:\n",
    "        print(f\"Training data for {model_name} is empty. Skipping training.\")\n",
    "        return None, [], -1.0  # model, features_used_list, best_cv_score\n",
    "\n",
    "    # Create CatBoost model with common parameters\n",
    "    catboost = cb.CatBoostRegressor(**CATBOOST_PARAMS_COMMON)\n",
    "    kf = KFold(n_splits=N_SPLITS_CV, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    print(f\"Performing RandomizedSearchCV for {model_name} with {X_train.shape[1]} features...\")\n",
    "    # Using RandomizedSearchCV instead of GridSearchCV for faster execution with more parameters\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=catboost, \n",
    "        param_distributions=CATBOOST_PARAM_GRID, \n",
    "        n_iter=30,\n",
    "        cv=kf, \n",
    "        scoring='r2', \n",
    "        verbose=1, \n",
    "        n_jobs=-1,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = random_search.best_estimator_\n",
    "    best_params = random_search.best_params_\n",
    "    best_cv_r2_score = random_search.best_score_\n",
    "    \n",
    "    print(f\"Best parameters for {model_name}: {best_params}\")\n",
    "    print(f\"Best cross-validated R² score for {model_name}: {best_cv_r2_score:.4f}\")\n",
    "    \n",
    "    # Optional: Feature importance analysis\n",
    "    if best_model is not None:\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'Importance': best_model.feature_importances_\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        print(\"\\nTop 10 Feature Importances:\")\n",
    "        print(feature_importance.head(10))\n",
    "        \n",
    "    return best_model, X_train.columns.tolist(), best_cv_r2_score\n",
    "\n",
    "def compare_models_and_predict(df, X_A_train, y_A_train, model_A, model_A_features, \n",
    "                              cv_r2_A, subset_A_indices, subset_A_early_imputed):\n",
    "    \"\"\"\n",
    "    Train CatBoost model, compare with LightGBM, and use the best model for imputation\n",
    "    \"\"\"\n",
    "    print(\"\\n--- STEP 3: Training CatBoost Model and Model Comparison ---\")\n",
    "    \n",
    "    # Train CatBoost model if training data is available\n",
    "    if not X_A_train.empty and not y_A_train.empty:\n",
    "        model_catboost, _, cv_r2_catboost = train_catboost_model(X_A_train, y_A_train, \"CatBoost Model\")\n",
    "        \n",
    "        # Compare model performances\n",
    "        print(\"\\n--- Model Performance Comparison ---\")\n",
    "        print(f\"LightGBM Model A CV R²: {cv_r2_A:.4f}\")\n",
    "        print(f\"CatBoost Model CV R²: {cv_r2_catboost:.4f}\")\n",
    "        \n",
    "        # Select the better model for imputation\n",
    "        if cv_r2_catboost > cv_r2_A:\n",
    "            best_model = model_catboost\n",
    "            best_model_name = \"CatBoost\"\n",
    "            best_cv_r2 = cv_r2_catboost\n",
    "        else:\n",
    "            best_model = model_A\n",
    "            best_model_name = \"LightGBM\"\n",
    "            best_cv_r2 = cv_r2_A\n",
    "            \n",
    "        print(f\"\\nThe better model is {best_model_name} with CV R² score: {best_cv_r2:.4f}.\")\n",
    "        \n",
    "        # Create an ensemble model for potentially better predictions\n",
    "        if model_A is not None and model_catboost is not None:\n",
    "            print(\"\\n--- Creating an ensemble model with weighted average ---\")\n",
    "            # Create a simple ensemble using weighted average based on CV scores\n",
    "            total_score = cv_r2_A + cv_r2_catboost\n",
    "            lgbm_weight = cv_r2_A / total_score\n",
    "            catboost_weight = cv_r2_catboost / total_score\n",
    "            print(f\"Ensemble weights: LightGBM = {lgbm_weight:.4f}, CatBoost = {catboost_weight:.4f}\")\n",
    "            \n",
    "            # Define ensemble prediction function\n",
    "            def ensemble_predict(X):\n",
    "                return (lgbm_weight * model_A.predict(X) + \n",
    "                        catboost_weight * model_catboost.predict(X))\n",
    "                \n",
    "            # Perform imputation if not already done in early imputation step\n",
    "            if not subset_A_early_imputed:\n",
    "                print(f\"\\n--- Using ensemble model to impute missing hydrostatic_pressure values ---\")\n",
    "                # Get rows with missing hydrostatic_pressure but available seafloor_pressure\n",
    "                missing_hydro_pressure_mask = df['hydrostatic_pressure'].isnull()\n",
    "                has_seafloor_mask = df['seafloor_pressure'].notnull()\n",
    "                to_impute_mask = missing_hydro_pressure_mask & has_seafloor_mask\n",
    "                to_impute_indices = df[to_impute_mask].index\n",
    "                \n",
    "                if len(to_impute_indices) > 0:\n",
    "                    # Prepare features for prediction\n",
    "                    to_impute_data_raw = df.loc[to_impute_indices].drop(columns=['hydrostatic_pressure'], errors='ignore').copy()\n",
    "                    to_impute_features, _ = preprocess_data_for_model(\n",
    "                        to_impute_data_raw, model_A_features, is_training_data=False, for_model_A=True\n",
    "                    )\n",
    "                    \n",
    "                    # Make predictions and impute missing values using ensemble\n",
    "                    predictions = ensemble_predict(to_impute_features)\n",
    "                    df.loc[to_impute_indices, 'hydrostatic_pressure'] = predictions\n",
    "                    print(f\"Imputed {len(predictions)} hydrostatic_pressure values using ensemble model.\")\n",
    "                else:\n",
    "                    print(\"No rows requiring imputation were found.\")\n",
    "            elif subset_A_early_imputed:\n",
    "                print(\"Imputation was already done in the early imputation step. Skipping additional imputation.\")\n",
    "        # If ensemble creation failed or not possible, use the best single model\n",
    "        elif not subset_A_early_imputed and best_model is not None:\n",
    "            print(f\"\\n--- Using {best_model_name} to impute missing hydrostatic_pressure values ---\")\n",
    "            # Get rows with missing hydrostatic_pressure but available seafloor_pressure\n",
    "            missing_hydro_pressure_mask = df['hydrostatic_pressure'].isnull()\n",
    "            has_seafloor_mask = df['seafloor_pressure'].notnull()\n",
    "            to_impute_mask = missing_hydro_pressure_mask & has_seafloor_mask\n",
    "            to_impute_indices = df[to_impute_mask].index\n",
    "            \n",
    "            if len(to_impute_indices) > 0:\n",
    "                # Prepare features for prediction\n",
    "                to_impute_data_raw = df.loc[to_impute_indices].drop(columns=['hydrostatic_pressure'], errors='ignore').copy()\n",
    "                to_impute_features, _ = preprocess_data_for_model(\n",
    "                    to_impute_data_raw, model_A_features, is_training_data=False, for_model_A=True\n",
    "                )\n",
    "                \n",
    "                # Make predictions and impute missing values\n",
    "                predictions = best_model.predict(to_impute_features)\n",
    "                df.loc[to_impute_indices, 'hydrostatic_pressure'] = predictions\n",
    "                print(f\"Imputed {len(predictions)} hydrostatic_pressure values using {best_model_name} model.\")\n",
    "            else:\n",
    "                print(\"No rows requiring imputation were found.\")\n",
    "        elif subset_A_early_imputed:\n",
    "            print(\"Imputation was already done in the early imputation step. Skipping additional imputation.\")\n",
    "        else:\n",
    "            print(\"No model available for imputation.\")\n",
    "    else:\n",
    "        print(\"No training data available. Skipping model training and imputation.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9bbb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 3: Training CatBoost Model and Model Comparison ---\n",
      "\n",
      "--- Training CatBoost Model ---\n",
      "Performing RandomizedSearchCV for CatBoost Model with 60 features...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Best parameters for CatBoost Model: {'subsample': 0.66, 'random_strength': 0.1, 'od_wait': 100, 'od_type': 'Iter', 'min_data_in_leaf': 20, 'learning_rate': 0.01, 'leaf_estimation_method': 'Newton', 'l2_leaf_reg': 1, 'iterations': 2000, 'grow_policy': 'SymmetricTree', 'depth': 10, 'border_count': 254, 'bagging_temperature': 1.0}\n",
      "Best cross-validated R² score for CatBoost Model: 0.9995\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "                                   Feature  Importance\n",
      "59                    seafloor_pressure_sq   48.414099\n",
      "6                        seafloor_pressure   45.005341\n",
      "4                  perceived_water_density    1.342056\n",
      "1                    water_temperature_50m    1.193448\n",
      "17          sediment_temperature_0_to_10cm    0.724335\n",
      "28                       thermal_emissions    0.442710\n",
      "36  dissolved_inorganic_carbon (µmol kg-1)    0.434748\n",
      "25                       downwelling_light    0.288752\n",
      "11                  dissolved_gas_pressure    0.273394\n",
      "37                                      pH    0.257835\n",
      "\n",
      "--- Model Performance Comparison ---\n",
      "LightGBM Model A CV R²: 0.9995\n",
      "CatBoost Model CV R²: 0.9995\n",
      "\n",
      "The better model is CatBoost with CV R² score: 0.9995.\n",
      "\n",
      "--- Creating an ensemble model with weighted average ---\n",
      "Ensemble weights: LightGBM = 0.5000, CatBoost = 0.5000\n",
      "Imputation was already done in the early imputation step. Skipping additional imputation.\n",
      "\n",
      "--- Saving imputed data to data/fitur_clean/hydrostatic1.csv ---\n",
      "Data successfully saved to data/fitur_clean/hydrostatic1.csv\n",
      "\n",
      "--- Summary ---\n",
      "Total rows in dataset: 21888\n",
      "Rows with NaN in hydrostatic_pressure initially: 6567\n",
      "Rows with NaN in hydrostatic_pressure after imputation: 1289\n",
      "Total rows imputed: 5278\n"
     ]
    }
   ],
   "source": [
    "# Execute Step 3: Compare models and perform final imputation\n",
    "df_with_imputation = compare_models_and_predict(\n",
    "    df=df,\n",
    "    X_A_train=X_A_train,\n",
    "    y_A_train=y_A_train,\n",
    "    model_A=model_A,\n",
    "    model_A_features=model_A_features,\n",
    "    cv_r2_A=cv_r2_A,\n",
    "    subset_A_indices=subset_A_indices,\n",
    "    subset_A_early_imputed=subset_A_early_imputed\n",
    ")\n",
    "\n",
    "# Save the imputed data\n",
    "output_path = 'data/fitur_clean/hydrostatic1.csv'\n",
    "print(f\"\\n--- Saving imputed data to {output_path} ---\")\n",
    "df_with_imputation.to_csv(output_path, index=False)\n",
    "print(f\"Data successfully saved to {output_path}\")\n",
    "\n",
    "# Print summary of imputation\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(f\"Total rows in dataset: {len(df_with_imputation)}\")\n",
    "print(f\"Rows with NaN in hydrostatic_pressure initially: {missing_hydro_pressure_mask_initial.sum()}\")\n",
    "print(f\"Rows with NaN in hydrostatic_pressure after imputation: {df_with_imputation['hydrostatic_pressure'].isnull().sum()}\")\n",
    "print(f\"Total rows imputed: {missing_hydro_pressure_mask_initial.sum() - df_with_imputation['hydrostatic_pressure'].isnull().sum()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analisis-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
